// Code generated by protoc-gen-go.
// source: serving/tensorflow_serving/servables/tensorflow/session_bundle_config.proto
// DO NOT EDIT!

/*
Package tensorflow_serving is a generated protocol buffer package.

It is generated from these files:
	serving/tensorflow_serving/servables/tensorflow/session_bundle_config.proto

It has these top-level messages:
	SessionBundleConfig
	BatchingParameters
*/
package tensorflow_serving

import proto "github.com/golang/protobuf/proto"
import fmt "fmt"
import math "math"
import google_protobuf "github.com/golang/protobuf/ptypes/wrappers"
import tensorflow17 "tensorflow/core/protobuf"
import tensorflow18 "tensorflow/core/protobuf"

// Reference imports to suppress errors if they are not otherwise used.
var _ = proto.Marshal
var _ = fmt.Errorf
var _ = math.Inf

// This is a compile-time assertion to ensure that this generated file
// is compatible with the proto package it is being compiled against.
// A compilation error at this line likely means your copy of the
// proto package needs to be updated.
const _ = proto.ProtoPackageIsVersion2 // please upgrade the proto package

// Configuration parameters for a SessionBundle, with optional batching.
type SessionBundleConfig struct {
	// The TensorFlow runtime to connect to.
	// See full documentation in tensorflow/core/public/session_options.h.
	//
	// For single machine serving, we recommend using the empty string "", which
	// will configure the local TensorFlow runtime implementation. This provides
	// the best isolation currently available across multiple Session servables.
	SessionTarget string `protobuf:"bytes,1,opt,name=session_target,json=sessionTarget" json:"session_target,omitempty"`
	// TensorFlow Session configuration options.
	// See details at tensorflow/core/protobuf/config.proto.
	SessionConfig *tensorflow17.ConfigProto `protobuf:"bytes,2,opt,name=session_config,json=sessionConfig" json:"session_config,omitempty"`
	// If set, each emitted session is wrapped with a layer that schedules Run()
	// calls in batches. The batching layer is transparent to the client
	// (implements the tensorflow::Session API).
	//
	// IMPORTANT: With batching enabled, client threads will spend most of their
	// time blocked on Session::Run() calls, waiting for enough peer threads to
	// also call Session::Run() such that a large batch can be formed. For good
	// throughput, we recommend setting the number of client threads equal to
	// roughly twice the maximum batch size ('max_batch_size' below).
	//
	// The batching layer uses a SharedBatchScheduler to coordinate batching
	// across multiple session servables emitted by this source adapter. A
	// BatchSchedulerRetrier is added on top of each batching session.
	BatchingParameters *BatchingParameters `protobuf:"bytes,3,opt,name=batching_parameters,json=batchingParameters" json:"batching_parameters,omitempty"`
	// If set, session run calls use a separate threadpool for restore and init
	// ops as part of loading the session-bundle. The value of this field should
	// correspond to the index of the tensorflow::ThreadPoolOptionProto defined as
	// part of `session_config.session_inter_op_thread_pool`.
	SessionRunLoadThreadpoolIndex *google_protobuf.Int32Value `protobuf:"bytes,4,opt,name=session_run_load_threadpool_index,json=sessionRunLoadThreadpoolIndex" json:"session_run_load_threadpool_index,omitempty"`
	// EXPERIMENTAL. THIS FIELD MAY CHANGE OR GO AWAY. USE WITH CAUTION.
	//
	// Input tensors to append to every Session::Run() call.
	ExperimentalFixedInputTensors []*tensorflow18.NamedTensorProto `protobuf:"bytes,778,rep,name=experimental_fixed_input_tensors,json=experimentalFixedInputTensors" json:"experimental_fixed_input_tensors,omitempty"`
}

func (m *SessionBundleConfig) Reset()                    { *m = SessionBundleConfig{} }
func (m *SessionBundleConfig) String() string            { return proto.CompactTextString(m) }
func (*SessionBundleConfig) ProtoMessage()               {}
func (*SessionBundleConfig) Descriptor() ([]byte, []int) { return fileDescriptor0, []int{0} }

func (m *SessionBundleConfig) GetSessionTarget() string {
	if m != nil {
		return m.SessionTarget
	}
	return ""
}

func (m *SessionBundleConfig) GetSessionConfig() *tensorflow17.ConfigProto {
	if m != nil {
		return m.SessionConfig
	}
	return nil
}

func (m *SessionBundleConfig) GetBatchingParameters() *BatchingParameters {
	if m != nil {
		return m.BatchingParameters
	}
	return nil
}

func (m *SessionBundleConfig) GetSessionRunLoadThreadpoolIndex() *google_protobuf.Int32Value {
	if m != nil {
		return m.SessionRunLoadThreadpoolIndex
	}
	return nil
}

func (m *SessionBundleConfig) GetExperimentalFixedInputTensors() []*tensorflow18.NamedTensorProto {
	if m != nil {
		return m.ExperimentalFixedInputTensors
	}
	return nil
}

// Batching parameters. Each individual parameter is optional. If omitted, the
// default value from the relevant batching config struct (SharedBatchScheduler
// ::Options or BatchSchedulerRetrier::Options) is used.
type BatchingParameters struct {
	// The maximum size of each batch.
	//
	// IMPORTANT: As discussed above, use 'max_batch_size * 2' client threads to
	// achieve high throughput with batching.
	MaxBatchSize *google_protobuf.Int64Value `protobuf:"bytes,1,opt,name=max_batch_size,json=maxBatchSize" json:"max_batch_size,omitempty"`
	// If a task has been enqueued for this amount of time (in microseconds), and
	// a thread is available, the scheduler will immediately form a batch from
	// enqueued tasks and assign the batch to the thread for processing, even if
	// the batch's size is below 'max_batch_size'.
	BatchTimeoutMicros *google_protobuf.Int64Value `protobuf:"bytes,2,opt,name=batch_timeout_micros,json=batchTimeoutMicros" json:"batch_timeout_micros,omitempty"`
	// The maximum length of the queue, in terms of the number of batches. (A
	// batch that has been scheduled on a thread is considered to have been
	// removed from the queue.)
	MaxEnqueuedBatches *google_protobuf.Int64Value `protobuf:"bytes,3,opt,name=max_enqueued_batches,json=maxEnqueuedBatches" json:"max_enqueued_batches,omitempty"`
	// The number of threads to use to process batches.
	// Must be >= 1, and should be tuned carefully.
	NumBatchThreads *google_protobuf.Int64Value `protobuf:"bytes,4,opt,name=num_batch_threads,json=numBatchThreads" json:"num_batch_threads,omitempty"`
	// The name to use for the pool of batch threads.
	ThreadPoolName *google_protobuf.StringValue `protobuf:"bytes,5,opt,name=thread_pool_name,json=threadPoolName" json:"thread_pool_name,omitempty"`
	// The allowed batch sizes. (Ignored if left empty.)
	// Requirements:
	//  - The entries must be in increasing order.
	//  - The final entry must equal 'max_batch_size'.
	AllowedBatchSizes []int64 `protobuf:"varint,6,rep,packed,name=allowed_batch_sizes,json=allowedBatchSizes" json:"allowed_batch_sizes,omitempty"`
}

func (m *BatchingParameters) Reset()                    { *m = BatchingParameters{} }
func (m *BatchingParameters) String() string            { return proto.CompactTextString(m) }
func (*BatchingParameters) ProtoMessage()               {}
func (*BatchingParameters) Descriptor() ([]byte, []int) { return fileDescriptor0, []int{1} }

func (m *BatchingParameters) GetMaxBatchSize() *google_protobuf.Int64Value {
	if m != nil {
		return m.MaxBatchSize
	}
	return nil
}

func (m *BatchingParameters) GetBatchTimeoutMicros() *google_protobuf.Int64Value {
	if m != nil {
		return m.BatchTimeoutMicros
	}
	return nil
}

func (m *BatchingParameters) GetMaxEnqueuedBatches() *google_protobuf.Int64Value {
	if m != nil {
		return m.MaxEnqueuedBatches
	}
	return nil
}

func (m *BatchingParameters) GetNumBatchThreads() *google_protobuf.Int64Value {
	if m != nil {
		return m.NumBatchThreads
	}
	return nil
}

func (m *BatchingParameters) GetThreadPoolName() *google_protobuf.StringValue {
	if m != nil {
		return m.ThreadPoolName
	}
	return nil
}

func (m *BatchingParameters) GetAllowedBatchSizes() []int64 {
	if m != nil {
		return m.AllowedBatchSizes
	}
	return nil
}

func init() {
	proto.RegisterType((*SessionBundleConfig)(nil), "tensorflow.serving.SessionBundleConfig")
	proto.RegisterType((*BatchingParameters)(nil), "tensorflow.serving.BatchingParameters")
}

func init() {
	proto.RegisterFile("serving/tensorflow_serving/servables/tensorflow/session_bundle_config.proto", fileDescriptor0)
}

var fileDescriptor0 = []byte{
	// 526 bytes of a gzipped FileDescriptorProto
	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0x7c, 0x93, 0xed, 0x6a, 0x13, 0x4f,
	0x18, 0xc5, 0xc9, 0x3f, 0x7f, 0x0b, 0x4e, 0x35, 0xda, 0x89, 0xe0, 0x12, 0x5b, 0x89, 0x85, 0x4a,
	0x40, 0xd8, 0x40, 0x2a, 0x7e, 0x14, 0x8c, 0x58, 0x09, 0x5a, 0x09, 0x49, 0xd0, 0x8f, 0xc3, 0x6c,
	0xf6, 0xc9, 0x76, 0x60, 0x76, 0x66, 0x9d, 0x17, 0xb3, 0xf4, 0x12, 0xbc, 0x0e, 0xaf, 0xc1, 0xeb,
	0x93, 0x79, 0xd9, 0x74, 0x21, 0x34, 0x9f, 0x42, 0x66, 0xce, 0xf9, 0xcd, 0xf3, 0x9c, 0xc3, 0xa2,
	0x2f, 0x1a, 0xd4, 0x2f, 0x26, 0x8a, 0xb1, 0x01, 0xa1, 0xa5, 0xda, 0x70, 0xb9, 0x25, 0xcd, 0x91,
	0xfb, 0xa5, 0x19, 0x07, 0xdd, 0xba, 0x1c, 0x6b, 0xd0, 0x9a, 0x49, 0x41, 0x32, 0x2b, 0x72, 0x0e,
	0x64, 0x2d, 0xc5, 0x86, 0x15, 0x69, 0xa5, 0xa4, 0x91, 0x18, 0xdf, 0xe9, 0xd2, 0x08, 0x19, 0xbc,
	0x2c, 0xa4, 0x2c, 0x38, 0x8c, 0xbd, 0x22, 0xb3, 0x9b, 0xf1, 0x56, 0xd1, 0xaa, 0x02, 0xa5, 0x83,
	0x67, 0x70, 0xd1, 0x62, 0xaf, 0xa5, 0x6a, 0x09, 0xdb, 0xe8, 0xc1, 0x9b, 0x7b, 0x65, 0x82, 0x96,
	0x90, 0x93, 0x70, 0x1d, 0xc4, 0xe7, 0x7f, 0xba, 0xa8, 0xbf, 0x0c, 0x73, 0x4e, 0xfd, 0x98, 0x1f,
	0x3d, 0x0a, 0x5f, 0xa0, 0x5e, 0x33, 0xbe, 0xa1, 0xaa, 0x00, 0x93, 0x74, 0x86, 0x9d, 0xd1, 0xc3,
	0xc5, 0xe3, 0x78, 0xba, 0xf2, 0x87, 0xf8, 0xfd, 0x9d, 0x2c, 0xcc, 0x90, 0xfc, 0x37, 0xec, 0x8c,
	0x8e, 0x27, 0xcf, 0xd3, 0xd6, 0x7e, 0x01, 0x39, 0x77, 0xef, 0xed, 0xfc, 0xf1, 0x99, 0x1f, 0xa8,
	0x9f, 0x51, 0xb3, 0xbe, 0x61, 0xa2, 0x20, 0x15, 0x55, 0xb4, 0x04, 0x03, 0x4a, 0x27, 0x5d, 0x0f,
	0x79, 0x9d, 0xee, 0x87, 0x94, 0x4e, 0xa3, 0x7c, 0xbe, 0x53, 0x2f, 0x70, 0xb6, 0x77, 0x86, 0x01,
	0xbd, 0x6a, 0x06, 0x53, 0x56, 0x10, 0x2e, 0x69, 0x4e, 0xcc, 0x8d, 0x02, 0x9a, 0x57, 0x52, 0x72,
	0xc2, 0x44, 0x0e, 0x75, 0xf2, 0xbf, 0x7f, 0xe6, 0x45, 0x1a, 0x72, 0x4f, 0x9b, 0x9c, 0xd2, 0x99,
	0x30, 0x97, 0x93, 0xef, 0x94, 0x5b, 0x58, 0x9c, 0x45, 0xca, 0xc2, 0x8a, 0xaf, 0x92, 0xe6, 0xab,
	0x1d, 0x62, 0xe6, 0x08, 0x78, 0x83, 0x86, 0x50, 0x57, 0xa0, 0x58, 0x09, 0xc2, 0x50, 0x4e, 0x36,
	0xac, 0x86, 0x9c, 0x30, 0x51, 0x59, 0x13, 0x73, 0xd6, 0xc9, 0xef, 0xa3, 0x61, 0x77, 0x74, 0x3c,
	0x39, 0x6d, 0x6f, 0xf3, 0xcd, 0x35, 0xb1, 0xf2, 0xff, 0x43, 0x2e, 0x67, 0x6d, 0xcc, 0x95, 0xa3,
	0xcc, 0x1c, 0x24, 0x48, 0xf4, 0xf9, 0xdf, 0x2e, 0xc2, 0xfb, 0x9b, 0xe3, 0x0f, 0xa8, 0x57, 0xd2,
	0x9a, 0xf8, 0xfd, 0x89, 0x66, 0xb7, 0xe0, 0x5b, 0xba, 0x67, 0xa5, 0x77, 0x6f, 0xc3, 0x4a, 0x8f,
	0x4a, 0x5a, 0x7b, 0xd6, 0x92, 0xdd, 0x02, 0xbe, 0x46, 0xcf, 0x82, 0xdd, 0xb0, 0x12, 0xa4, 0x35,
	0xa4, 0x64, 0x6b, 0x25, 0x75, 0xec, 0xf1, 0x20, 0x28, 0xe4, 0xbe, 0x0a, 0xbe, 0x6b, 0x6f, 0x73,
	0x38, 0x37, 0x11, 0x88, 0x9f, 0x16, 0x2c, 0xe4, 0x61, 0x34, 0x68, 0x1a, 0x3d, 0x8c, 0x2b, 0x69,
	0xfd, 0x29, 0xfa, 0xa6, 0xc1, 0x86, 0x3f, 0xa3, 0x13, 0x61, 0xcb, 0xb8, 0x60, 0xe8, 0x4f, 0x1f,
	0xaa, 0xad, 0x61, 0x3d, 0x11, 0xb6, 0xf4, 0x8c, 0x50, 0x98, 0xc6, 0x57, 0xe8, 0x69, 0xb0, 0x13,
	0xdf, 0xbf, 0xfb, 0x12, 0x92, 0x07, 0x9e, 0x73, 0xba, 0xc7, 0x59, 0x1a, 0xc5, 0x44, 0x11, 0x40,
	0xbd, 0xe0, 0x9a, 0x4b, 0xc9, 0x5d, 0x67, 0x38, 0x45, 0x7d, 0xca, 0xb9, 0xdc, 0x36, 0xab, 0xf9,
	0xd4, 0x75, 0xe2, 0x2a, 0xee, 0x2e, 0x4e, 0xe2, 0xd5, 0x2e, 0x5d, 0x9d, 0x1d, 0x79, 0xea, 0xe5,
	0xbf, 0x00, 0x00, 0x00, 0xff, 0xff, 0x1c, 0xdb, 0x55, 0x20, 0x3d, 0x04, 0x00, 0x00,
}
