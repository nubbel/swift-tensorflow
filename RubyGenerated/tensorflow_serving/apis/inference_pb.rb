# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: tensorflow_serving/apis/inference.proto

require 'google/protobuf'

require 'tensorflow_serving/apis/classification_pb'
require 'tensorflow_serving/apis/input_pb'
require 'tensorflow_serving/apis/model_pb'
require 'tensorflow_serving/apis/regression_pb'
Google::Protobuf::DescriptorPool.generated_pool.build do
  add_message "tensorflow.serving.InferenceTask" do
    optional :model_spec, :message, 1, "tensorflow.serving.ModelSpec"
    optional :method_name, :string, 2
  end
  add_message "tensorflow.serving.InferenceResult" do
    optional :model_spec, :message, 1, "tensorflow.serving.ModelSpec"
    oneof :result do
      optional :classification_result, :message, 2, "tensorflow.serving.ClassificationResult"
      optional :regression_result, :message, 3, "tensorflow.serving.RegressionResult"
    end
  end
  add_message "tensorflow.serving.MultiInferenceRequest" do
    repeated :tasks, :message, 1, "tensorflow.serving.InferenceTask"
    optional :input, :message, 2, "tensorflow.serving.Input"
  end
  add_message "tensorflow.serving.MultiInferenceResponse" do
    repeated :results, :message, 1, "tensorflow.serving.InferenceResult"
  end
end

module Tensorflow
  module Serving
    InferenceTask = Google::Protobuf::DescriptorPool.generated_pool.lookup("tensorflow.serving.InferenceTask").msgclass
    InferenceResult = Google::Protobuf::DescriptorPool.generated_pool.lookup("tensorflow.serving.InferenceResult").msgclass
    MultiInferenceRequest = Google::Protobuf::DescriptorPool.generated_pool.lookup("tensorflow.serving.MultiInferenceRequest").msgclass
    MultiInferenceResponse = Google::Protobuf::DescriptorPool.generated_pool.lookup("tensorflow.serving.MultiInferenceResponse").msgclass
  end
end
