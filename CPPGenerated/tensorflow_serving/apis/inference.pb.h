// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: tensorflow_serving/apis/inference.proto

#ifndef PROTOBUF_tensorflow_5fserving_2fapis_2finference_2eproto__INCLUDED
#define PROTOBUF_tensorflow_5fserving_2fapis_2finference_2eproto__INCLUDED

#include <string>

#include <google/protobuf/stubs/common.h>

#if GOOGLE_PROTOBUF_VERSION < 3002000
#error This file was generated by a newer version of protoc which is
#error incompatible with your Protocol Buffer headers.  Please update
#error your headers.
#endif
#if 3002000 < GOOGLE_PROTOBUF_MIN_PROTOC_VERSION
#error This file was generated by an older version of protoc which is
#error incompatible with your Protocol Buffer headers.  Please
#error regenerate this file with a newer version of protoc.
#endif

#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/arena.h>
#include <google/protobuf/arenastring.h>
#include <google/protobuf/generated_message_util.h>
#include <google/protobuf/metadata.h>
#include <google/protobuf/message.h>
#include <google/protobuf/repeated_field.h>  // IWYU pragma: export
#include <google/protobuf/extension_set.h>  // IWYU pragma: export
#include <google/protobuf/unknown_field_set.h>
#include "tensorflow_serving/apis/classification.pb.h"
#include "tensorflow_serving/apis/input.pb.h"
#include "tensorflow_serving/apis/model.pb.h"
#include "tensorflow_serving/apis/regression.pb.h"
// @@protoc_insertion_point(includes)
namespace tensorflow {
namespace serving {
class Class;
class ClassDefaultTypeInternal;
extern ClassDefaultTypeInternal _Class_default_instance_;
class ClassificationRequest;
class ClassificationRequestDefaultTypeInternal;
extern ClassificationRequestDefaultTypeInternal _ClassificationRequest_default_instance_;
class ClassificationResponse;
class ClassificationResponseDefaultTypeInternal;
extern ClassificationResponseDefaultTypeInternal _ClassificationResponse_default_instance_;
class ClassificationResult;
class ClassificationResultDefaultTypeInternal;
extern ClassificationResultDefaultTypeInternal _ClassificationResult_default_instance_;
class Classifications;
class ClassificationsDefaultTypeInternal;
extern ClassificationsDefaultTypeInternal _Classifications_default_instance_;
class ExampleList;
class ExampleListDefaultTypeInternal;
extern ExampleListDefaultTypeInternal _ExampleList_default_instance_;
class ExampleListWithContext;
class ExampleListWithContextDefaultTypeInternal;
extern ExampleListWithContextDefaultTypeInternal _ExampleListWithContext_default_instance_;
class InferenceResult;
class InferenceResultDefaultTypeInternal;
extern InferenceResultDefaultTypeInternal _InferenceResult_default_instance_;
class InferenceTask;
class InferenceTaskDefaultTypeInternal;
extern InferenceTaskDefaultTypeInternal _InferenceTask_default_instance_;
class Input;
class InputDefaultTypeInternal;
extern InputDefaultTypeInternal _Input_default_instance_;
class ModelSpec;
class ModelSpecDefaultTypeInternal;
extern ModelSpecDefaultTypeInternal _ModelSpec_default_instance_;
class MultiInferenceRequest;
class MultiInferenceRequestDefaultTypeInternal;
extern MultiInferenceRequestDefaultTypeInternal _MultiInferenceRequest_default_instance_;
class MultiInferenceResponse;
class MultiInferenceResponseDefaultTypeInternal;
extern MultiInferenceResponseDefaultTypeInternal _MultiInferenceResponse_default_instance_;
class Regression;
class RegressionDefaultTypeInternal;
extern RegressionDefaultTypeInternal _Regression_default_instance_;
class RegressionRequest;
class RegressionRequestDefaultTypeInternal;
extern RegressionRequestDefaultTypeInternal _RegressionRequest_default_instance_;
class RegressionResponse;
class RegressionResponseDefaultTypeInternal;
extern RegressionResponseDefaultTypeInternal _RegressionResponse_default_instance_;
class RegressionResult;
class RegressionResultDefaultTypeInternal;
extern RegressionResultDefaultTypeInternal _RegressionResult_default_instance_;
}  // namespace serving
}  // namespace tensorflow

namespace tensorflow {
namespace serving {

namespace protobuf_tensorflow_5fserving_2fapis_2finference_2eproto {
// Internal implementation detail -- do not call these.
struct TableStruct {
  static const ::google::protobuf::uint32 offsets[];
  static void InitDefaultsImpl();
  static void Shutdown();
};
void AddDescriptors();
void InitDefaults();
}  // namespace protobuf_tensorflow_5fserving_2fapis_2finference_2eproto

// ===================================================================

class InferenceTask : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.serving.InferenceTask) */ {
 public:
  InferenceTask();
  virtual ~InferenceTask();

  InferenceTask(const InferenceTask& from);

  inline InferenceTask& operator=(const InferenceTask& from) {
    CopyFrom(from);
    return *this;
  }

  inline ::google::protobuf::Arena* GetArena() const PROTOBUF_FINAL {
    return GetArenaNoVirtual();
  }
  inline void* GetMaybeArenaPointer() const PROTOBUF_FINAL {
    return MaybeArenaPtr();
  }
  static const ::google::protobuf::Descriptor* descriptor();
  static const InferenceTask& default_instance();

  static inline const InferenceTask* internal_default_instance() {
    return reinterpret_cast<const InferenceTask*>(
               &_InferenceTask_default_instance_);
  }

  void UnsafeArenaSwap(InferenceTask* other);
  void Swap(InferenceTask* other);

  // implements Message ----------------------------------------------

  inline InferenceTask* New() const PROTOBUF_FINAL { return New(NULL); }

  InferenceTask* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CopyFrom(const ::google::protobuf::Message& from) PROTOBUF_FINAL;
  void MergeFrom(const ::google::protobuf::Message& from) PROTOBUF_FINAL;
  void CopyFrom(const InferenceTask& from);
  void MergeFrom(const InferenceTask& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const PROTOBUF_FINAL;
  ::google::protobuf::uint8* SerializeWithCachedSizesToArray(::google::protobuf::uint8* output)
      const PROTOBUF_FINAL {
    return InternalSerializeWithCachedSizesToArray(
        ::google::protobuf::io::CodedOutputStream::IsDefaultSerializationDeterministic(), output);
  }
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const PROTOBUF_FINAL;
  void InternalSwap(InferenceTask* other);
  protected:
  explicit InferenceTask(::google::protobuf::Arena* arena);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::google::protobuf::Arena* arena);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _internal_metadata_.arena();
  }
  inline void* MaybeArenaPtr() const {
    return _internal_metadata_.raw_arena_ptr();
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // string method_name = 2;
  void clear_method_name();
  static const int kMethodNameFieldNumber = 2;
  const ::std::string& method_name() const;
  void set_method_name(const ::std::string& value);
  void set_method_name(const char* value);
  void set_method_name(const char* value, size_t size);
  ::std::string* mutable_method_name();
  ::std::string* release_method_name();
  void set_allocated_method_name(::std::string* method_name);
  ::std::string* unsafe_arena_release_method_name();
  void unsafe_arena_set_allocated_method_name(
      ::std::string* method_name);

  // .tensorflow.serving.ModelSpec model_spec = 1;
  bool has_model_spec() const;
  void clear_model_spec();
  static const int kModelSpecFieldNumber = 1;
  private:
  void _slow_mutable_model_spec();
  void _slow_set_allocated_model_spec(
      ::google::protobuf::Arena* message_arena, ::tensorflow::serving::ModelSpec** model_spec);
  ::tensorflow::serving::ModelSpec* _slow_release_model_spec();
  public:
  const ::tensorflow::serving::ModelSpec& model_spec() const;
  ::tensorflow::serving::ModelSpec* mutable_model_spec();
  ::tensorflow::serving::ModelSpec* release_model_spec();
  void set_allocated_model_spec(::tensorflow::serving::ModelSpec* model_spec);
  ::tensorflow::serving::ModelSpec* unsafe_arena_release_model_spec();
  void unsafe_arena_set_allocated_model_spec(
      ::tensorflow::serving::ModelSpec* model_spec);

  // @@protoc_insertion_point(class_scope:tensorflow.serving.InferenceTask)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  friend class ::google::protobuf::Arena;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::google::protobuf::internal::ArenaStringPtr method_name_;
  ::tensorflow::serving::ModelSpec* model_spec_;
  mutable int _cached_size_;
  friend struct  protobuf_tensorflow_5fserving_2fapis_2finference_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class InferenceResult : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.serving.InferenceResult) */ {
 public:
  InferenceResult();
  virtual ~InferenceResult();

  InferenceResult(const InferenceResult& from);

  inline InferenceResult& operator=(const InferenceResult& from) {
    CopyFrom(from);
    return *this;
  }

  inline ::google::protobuf::Arena* GetArena() const PROTOBUF_FINAL {
    return GetArenaNoVirtual();
  }
  inline void* GetMaybeArenaPointer() const PROTOBUF_FINAL {
    return MaybeArenaPtr();
  }
  static const ::google::protobuf::Descriptor* descriptor();
  static const InferenceResult& default_instance();

  enum ResultCase {
    kClassificationResult = 2,
    kRegressionResult = 3,
    RESULT_NOT_SET = 0,
  };

  static inline const InferenceResult* internal_default_instance() {
    return reinterpret_cast<const InferenceResult*>(
               &_InferenceResult_default_instance_);
  }

  void UnsafeArenaSwap(InferenceResult* other);
  void Swap(InferenceResult* other);

  // implements Message ----------------------------------------------

  inline InferenceResult* New() const PROTOBUF_FINAL { return New(NULL); }

  InferenceResult* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CopyFrom(const ::google::protobuf::Message& from) PROTOBUF_FINAL;
  void MergeFrom(const ::google::protobuf::Message& from) PROTOBUF_FINAL;
  void CopyFrom(const InferenceResult& from);
  void MergeFrom(const InferenceResult& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const PROTOBUF_FINAL;
  ::google::protobuf::uint8* SerializeWithCachedSizesToArray(::google::protobuf::uint8* output)
      const PROTOBUF_FINAL {
    return InternalSerializeWithCachedSizesToArray(
        ::google::protobuf::io::CodedOutputStream::IsDefaultSerializationDeterministic(), output);
  }
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const PROTOBUF_FINAL;
  void InternalSwap(InferenceResult* other);
  protected:
  explicit InferenceResult(::google::protobuf::Arena* arena);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::google::protobuf::Arena* arena);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _internal_metadata_.arena();
  }
  inline void* MaybeArenaPtr() const {
    return _internal_metadata_.raw_arena_ptr();
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .tensorflow.serving.ModelSpec model_spec = 1;
  bool has_model_spec() const;
  void clear_model_spec();
  static const int kModelSpecFieldNumber = 1;
  private:
  void _slow_mutable_model_spec();
  void _slow_set_allocated_model_spec(
      ::google::protobuf::Arena* message_arena, ::tensorflow::serving::ModelSpec** model_spec);
  ::tensorflow::serving::ModelSpec* _slow_release_model_spec();
  public:
  const ::tensorflow::serving::ModelSpec& model_spec() const;
  ::tensorflow::serving::ModelSpec* mutable_model_spec();
  ::tensorflow::serving::ModelSpec* release_model_spec();
  void set_allocated_model_spec(::tensorflow::serving::ModelSpec* model_spec);
  ::tensorflow::serving::ModelSpec* unsafe_arena_release_model_spec();
  void unsafe_arena_set_allocated_model_spec(
      ::tensorflow::serving::ModelSpec* model_spec);

  // .tensorflow.serving.ClassificationResult classification_result = 2;
  bool has_classification_result() const;
  void clear_classification_result();
  static const int kClassificationResultFieldNumber = 2;
  private:
  void _slow_mutable_classification_result();
  void _slow_set_allocated_classification_result(
      ::google::protobuf::Arena* message_arena, ::tensorflow::serving::ClassificationResult** classification_result);
  ::tensorflow::serving::ClassificationResult* _slow_release_classification_result();
  public:
  const ::tensorflow::serving::ClassificationResult& classification_result() const;
  ::tensorflow::serving::ClassificationResult* mutable_classification_result();
  ::tensorflow::serving::ClassificationResult* release_classification_result();
  void set_allocated_classification_result(::tensorflow::serving::ClassificationResult* classification_result);
  ::tensorflow::serving::ClassificationResult* unsafe_arena_release_classification_result();
  void unsafe_arena_set_allocated_classification_result(
      ::tensorflow::serving::ClassificationResult* classification_result);

  // .tensorflow.serving.RegressionResult regression_result = 3;
  bool has_regression_result() const;
  void clear_regression_result();
  static const int kRegressionResultFieldNumber = 3;
  private:
  void _slow_mutable_regression_result();
  void _slow_set_allocated_regression_result(
      ::google::protobuf::Arena* message_arena, ::tensorflow::serving::RegressionResult** regression_result);
  ::tensorflow::serving::RegressionResult* _slow_release_regression_result();
  public:
  const ::tensorflow::serving::RegressionResult& regression_result() const;
  ::tensorflow::serving::RegressionResult* mutable_regression_result();
  ::tensorflow::serving::RegressionResult* release_regression_result();
  void set_allocated_regression_result(::tensorflow::serving::RegressionResult* regression_result);
  ::tensorflow::serving::RegressionResult* unsafe_arena_release_regression_result();
  void unsafe_arena_set_allocated_regression_result(
      ::tensorflow::serving::RegressionResult* regression_result);

  ResultCase result_case() const;
  // @@protoc_insertion_point(class_scope:tensorflow.serving.InferenceResult)
 private:
  void set_has_classification_result();
  void set_has_regression_result();

  inline bool has_result() const;
  void clear_result();
  inline void clear_has_result();

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  friend class ::google::protobuf::Arena;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::tensorflow::serving::ModelSpec* model_spec_;
  union ResultUnion {
    ResultUnion() {}
    ::tensorflow::serving::ClassificationResult* classification_result_;
    ::tensorflow::serving::RegressionResult* regression_result_;
  } result_;
  mutable int _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend struct  protobuf_tensorflow_5fserving_2fapis_2finference_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class MultiInferenceRequest : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.serving.MultiInferenceRequest) */ {
 public:
  MultiInferenceRequest();
  virtual ~MultiInferenceRequest();

  MultiInferenceRequest(const MultiInferenceRequest& from);

  inline MultiInferenceRequest& operator=(const MultiInferenceRequest& from) {
    CopyFrom(from);
    return *this;
  }

  inline ::google::protobuf::Arena* GetArena() const PROTOBUF_FINAL {
    return GetArenaNoVirtual();
  }
  inline void* GetMaybeArenaPointer() const PROTOBUF_FINAL {
    return MaybeArenaPtr();
  }
  static const ::google::protobuf::Descriptor* descriptor();
  static const MultiInferenceRequest& default_instance();

  static inline const MultiInferenceRequest* internal_default_instance() {
    return reinterpret_cast<const MultiInferenceRequest*>(
               &_MultiInferenceRequest_default_instance_);
  }

  void UnsafeArenaSwap(MultiInferenceRequest* other);
  void Swap(MultiInferenceRequest* other);

  // implements Message ----------------------------------------------

  inline MultiInferenceRequest* New() const PROTOBUF_FINAL { return New(NULL); }

  MultiInferenceRequest* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CopyFrom(const ::google::protobuf::Message& from) PROTOBUF_FINAL;
  void MergeFrom(const ::google::protobuf::Message& from) PROTOBUF_FINAL;
  void CopyFrom(const MultiInferenceRequest& from);
  void MergeFrom(const MultiInferenceRequest& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const PROTOBUF_FINAL;
  ::google::protobuf::uint8* SerializeWithCachedSizesToArray(::google::protobuf::uint8* output)
      const PROTOBUF_FINAL {
    return InternalSerializeWithCachedSizesToArray(
        ::google::protobuf::io::CodedOutputStream::IsDefaultSerializationDeterministic(), output);
  }
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const PROTOBUF_FINAL;
  void InternalSwap(MultiInferenceRequest* other);
  protected:
  explicit MultiInferenceRequest(::google::protobuf::Arena* arena);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::google::protobuf::Arena* arena);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _internal_metadata_.arena();
  }
  inline void* MaybeArenaPtr() const {
    return _internal_metadata_.raw_arena_ptr();
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated .tensorflow.serving.InferenceTask tasks = 1;
  int tasks_size() const;
  void clear_tasks();
  static const int kTasksFieldNumber = 1;
  const ::tensorflow::serving::InferenceTask& tasks(int index) const;
  ::tensorflow::serving::InferenceTask* mutable_tasks(int index);
  ::tensorflow::serving::InferenceTask* add_tasks();
  ::google::protobuf::RepeatedPtrField< ::tensorflow::serving::InferenceTask >*
      mutable_tasks();
  const ::google::protobuf::RepeatedPtrField< ::tensorflow::serving::InferenceTask >&
      tasks() const;

  // .tensorflow.serving.Input input = 2;
  bool has_input() const;
  void clear_input();
  static const int kInputFieldNumber = 2;
  private:
  void _slow_mutable_input();
  void _slow_set_allocated_input(
      ::google::protobuf::Arena* message_arena, ::tensorflow::serving::Input** input);
  ::tensorflow::serving::Input* _slow_release_input();
  public:
  const ::tensorflow::serving::Input& input() const;
  ::tensorflow::serving::Input* mutable_input();
  ::tensorflow::serving::Input* release_input();
  void set_allocated_input(::tensorflow::serving::Input* input);
  ::tensorflow::serving::Input* unsafe_arena_release_input();
  void unsafe_arena_set_allocated_input(
      ::tensorflow::serving::Input* input);

  // @@protoc_insertion_point(class_scope:tensorflow.serving.MultiInferenceRequest)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  friend class ::google::protobuf::Arena;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::google::protobuf::RepeatedPtrField< ::tensorflow::serving::InferenceTask > tasks_;
  ::tensorflow::serving::Input* input_;
  mutable int _cached_size_;
  friend struct  protobuf_tensorflow_5fserving_2fapis_2finference_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class MultiInferenceResponse : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.serving.MultiInferenceResponse) */ {
 public:
  MultiInferenceResponse();
  virtual ~MultiInferenceResponse();

  MultiInferenceResponse(const MultiInferenceResponse& from);

  inline MultiInferenceResponse& operator=(const MultiInferenceResponse& from) {
    CopyFrom(from);
    return *this;
  }

  inline ::google::protobuf::Arena* GetArena() const PROTOBUF_FINAL {
    return GetArenaNoVirtual();
  }
  inline void* GetMaybeArenaPointer() const PROTOBUF_FINAL {
    return MaybeArenaPtr();
  }
  static const ::google::protobuf::Descriptor* descriptor();
  static const MultiInferenceResponse& default_instance();

  static inline const MultiInferenceResponse* internal_default_instance() {
    return reinterpret_cast<const MultiInferenceResponse*>(
               &_MultiInferenceResponse_default_instance_);
  }

  void UnsafeArenaSwap(MultiInferenceResponse* other);
  void Swap(MultiInferenceResponse* other);

  // implements Message ----------------------------------------------

  inline MultiInferenceResponse* New() const PROTOBUF_FINAL { return New(NULL); }

  MultiInferenceResponse* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CopyFrom(const ::google::protobuf::Message& from) PROTOBUF_FINAL;
  void MergeFrom(const ::google::protobuf::Message& from) PROTOBUF_FINAL;
  void CopyFrom(const MultiInferenceResponse& from);
  void MergeFrom(const MultiInferenceResponse& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const PROTOBUF_FINAL;
  ::google::protobuf::uint8* SerializeWithCachedSizesToArray(::google::protobuf::uint8* output)
      const PROTOBUF_FINAL {
    return InternalSerializeWithCachedSizesToArray(
        ::google::protobuf::io::CodedOutputStream::IsDefaultSerializationDeterministic(), output);
  }
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const PROTOBUF_FINAL;
  void InternalSwap(MultiInferenceResponse* other);
  protected:
  explicit MultiInferenceResponse(::google::protobuf::Arena* arena);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::google::protobuf::Arena* arena);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _internal_metadata_.arena();
  }
  inline void* MaybeArenaPtr() const {
    return _internal_metadata_.raw_arena_ptr();
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated .tensorflow.serving.InferenceResult results = 1;
  int results_size() const;
  void clear_results();
  static const int kResultsFieldNumber = 1;
  const ::tensorflow::serving::InferenceResult& results(int index) const;
  ::tensorflow::serving::InferenceResult* mutable_results(int index);
  ::tensorflow::serving::InferenceResult* add_results();
  ::google::protobuf::RepeatedPtrField< ::tensorflow::serving::InferenceResult >*
      mutable_results();
  const ::google::protobuf::RepeatedPtrField< ::tensorflow::serving::InferenceResult >&
      results() const;

  // @@protoc_insertion_point(class_scope:tensorflow.serving.MultiInferenceResponse)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  friend class ::google::protobuf::Arena;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::google::protobuf::RepeatedPtrField< ::tensorflow::serving::InferenceResult > results_;
  mutable int _cached_size_;
  friend struct  protobuf_tensorflow_5fserving_2fapis_2finference_2eproto::TableStruct;
};
// ===================================================================


// ===================================================================

#if !PROTOBUF_INLINE_NOT_IN_HEADERS
// InferenceTask

// .tensorflow.serving.ModelSpec model_spec = 1;
inline bool InferenceTask::has_model_spec() const {
  return this != internal_default_instance() && model_spec_ != NULL;
}
inline void InferenceTask::clear_model_spec() {
  if (GetArenaNoVirtual() == NULL && model_spec_ != NULL) delete model_spec_;
  model_spec_ = NULL;
}
inline const ::tensorflow::serving::ModelSpec& InferenceTask::model_spec() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.InferenceTask.model_spec)
  return model_spec_ != NULL ? *model_spec_
                         : *::tensorflow::serving::ModelSpec::internal_default_instance();
}
inline ::tensorflow::serving::ModelSpec* InferenceTask::mutable_model_spec() {
  
  if (model_spec_ == NULL) {
    _slow_mutable_model_spec();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.InferenceTask.model_spec)
  return model_spec_;
}
inline ::tensorflow::serving::ModelSpec* InferenceTask::release_model_spec() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.InferenceTask.model_spec)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_model_spec();
  } else {
    ::tensorflow::serving::ModelSpec* temp = model_spec_;
    model_spec_ = NULL;
    return temp;
  }
}
inline  void InferenceTask::set_allocated_model_spec(::tensorflow::serving::ModelSpec* model_spec) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete model_spec_;
  }
  if (model_spec != NULL) {
    _slow_set_allocated_model_spec(message_arena, &model_spec);
  }
  model_spec_ = model_spec;
  if (model_spec) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.InferenceTask.model_spec)
}

// string method_name = 2;
inline void InferenceTask::clear_method_name() {
  method_name_.ClearToEmpty(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), GetArenaNoVirtual());
}
inline const ::std::string& InferenceTask::method_name() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.InferenceTask.method_name)
  return method_name_.Get();
}
inline void InferenceTask::set_method_name(const ::std::string& value) {
  
  method_name_.Set(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value, GetArenaNoVirtual());
  // @@protoc_insertion_point(field_set:tensorflow.serving.InferenceTask.method_name)
}
inline void InferenceTask::set_method_name(const char* value) {
  
  method_name_.Set(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value),
              GetArenaNoVirtual());
  // @@protoc_insertion_point(field_set_char:tensorflow.serving.InferenceTask.method_name)
}
inline void InferenceTask::set_method_name(const char* value,
    size_t size) {
  
  method_name_.Set(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(
      reinterpret_cast<const char*>(value), size), GetArenaNoVirtual());
  // @@protoc_insertion_point(field_set_pointer:tensorflow.serving.InferenceTask.method_name)
}
inline ::std::string* InferenceTask::mutable_method_name() {
  
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.InferenceTask.method_name)
  return method_name_.Mutable(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), GetArenaNoVirtual());
}
inline ::std::string* InferenceTask::release_method_name() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.InferenceTask.method_name)
  
  return method_name_.Release(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), GetArenaNoVirtual());
}
inline ::std::string* InferenceTask::unsafe_arena_release_method_name() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.InferenceTask.method_name)
  GOOGLE_DCHECK(GetArenaNoVirtual() != NULL);
  
  return method_name_.UnsafeArenaRelease(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      GetArenaNoVirtual());
}
inline void InferenceTask::set_allocated_method_name(::std::string* method_name) {
  if (method_name != NULL) {
    
  } else {
    
  }
  method_name_.SetAllocated(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), method_name,
      GetArenaNoVirtual());
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.InferenceTask.method_name)
}
inline void InferenceTask::unsafe_arena_set_allocated_method_name(
    ::std::string* method_name) {
  GOOGLE_DCHECK(GetArenaNoVirtual() != NULL);
  if (method_name != NULL) {
    
  } else {
    
  }
  method_name_.UnsafeArenaSetAllocated(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      method_name, GetArenaNoVirtual());
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.InferenceTask.method_name)
}

// -------------------------------------------------------------------

// InferenceResult

// .tensorflow.serving.ModelSpec model_spec = 1;
inline bool InferenceResult::has_model_spec() const {
  return this != internal_default_instance() && model_spec_ != NULL;
}
inline void InferenceResult::clear_model_spec() {
  if (GetArenaNoVirtual() == NULL && model_spec_ != NULL) delete model_spec_;
  model_spec_ = NULL;
}
inline const ::tensorflow::serving::ModelSpec& InferenceResult::model_spec() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.InferenceResult.model_spec)
  return model_spec_ != NULL ? *model_spec_
                         : *::tensorflow::serving::ModelSpec::internal_default_instance();
}
inline ::tensorflow::serving::ModelSpec* InferenceResult::mutable_model_spec() {
  
  if (model_spec_ == NULL) {
    _slow_mutable_model_spec();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.InferenceResult.model_spec)
  return model_spec_;
}
inline ::tensorflow::serving::ModelSpec* InferenceResult::release_model_spec() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.InferenceResult.model_spec)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_model_spec();
  } else {
    ::tensorflow::serving::ModelSpec* temp = model_spec_;
    model_spec_ = NULL;
    return temp;
  }
}
inline  void InferenceResult::set_allocated_model_spec(::tensorflow::serving::ModelSpec* model_spec) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete model_spec_;
  }
  if (model_spec != NULL) {
    _slow_set_allocated_model_spec(message_arena, &model_spec);
  }
  model_spec_ = model_spec;
  if (model_spec) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.InferenceResult.model_spec)
}

// .tensorflow.serving.ClassificationResult classification_result = 2;
inline bool InferenceResult::has_classification_result() const {
  return result_case() == kClassificationResult;
}
inline void InferenceResult::set_has_classification_result() {
  _oneof_case_[0] = kClassificationResult;
}
inline void InferenceResult::clear_classification_result() {
  if (has_classification_result()) {
    if (GetArenaNoVirtual() == NULL) {
      delete result_.classification_result_;
    }
    clear_has_result();
  }
}
inline  const ::tensorflow::serving::ClassificationResult& InferenceResult::classification_result() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.InferenceResult.classification_result)
  return has_classification_result()
      ? *result_.classification_result_
      : ::tensorflow::serving::ClassificationResult::default_instance();
}
inline ::tensorflow::serving::ClassificationResult* InferenceResult::mutable_classification_result() {
  if (!has_classification_result()) {
    clear_result();
    set_has_classification_result();
    result_.classification_result_ = 
      ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::ClassificationResult >(
      GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.InferenceResult.classification_result)
  return result_.classification_result_;
}
inline ::tensorflow::serving::ClassificationResult* InferenceResult::release_classification_result() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.InferenceResult.classification_result)
  if (has_classification_result()) {
    clear_has_result();
    if (GetArenaNoVirtual() != NULL) {
      ::tensorflow::serving::ClassificationResult* temp = new ::tensorflow::serving::ClassificationResult(*result_.classification_result_);
      result_.classification_result_ = NULL;
      return temp;
    } else {
      ::tensorflow::serving::ClassificationResult* temp = result_.classification_result_;
      result_.classification_result_ = NULL;
      return temp;
    }
  } else {
    return NULL;
  }
}
inline void InferenceResult::set_allocated_classification_result(::tensorflow::serving::ClassificationResult* classification_result) {
  clear_result();
  if (classification_result) {
    if (GetArenaNoVirtual() != NULL &&
        ::google::protobuf::Arena::GetArena(classification_result) == NULL) {
      GetArenaNoVirtual()->Own(classification_result);
    } else if (GetArenaNoVirtual() !=
               ::google::protobuf::Arena::GetArena(classification_result)) {
      ::tensorflow::serving::ClassificationResult* new_classification_result = 
          ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::ClassificationResult >(
          GetArenaNoVirtual());
      new_classification_result->CopyFrom(*classification_result);
      classification_result = new_classification_result;
    }
    set_has_classification_result();
    result_.classification_result_ = classification_result;
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.InferenceResult.classification_result)
}
inline  ::tensorflow::serving::ClassificationResult* InferenceResult::unsafe_arena_release_classification_result() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.InferenceResult.classification_result)
  if (has_classification_result()) {
    clear_has_result();
    ::tensorflow::serving::ClassificationResult* temp = result_.classification_result_;
    result_.classification_result_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline  void InferenceResult::unsafe_arena_set_allocated_classification_result(::tensorflow::serving::ClassificationResult* classification_result) {
  clear_result();
  if (classification_result) {
    set_has_classification_result();
    result_.classification_result_ = classification_result;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.InferenceResult.classification_result)
}

// .tensorflow.serving.RegressionResult regression_result = 3;
inline bool InferenceResult::has_regression_result() const {
  return result_case() == kRegressionResult;
}
inline void InferenceResult::set_has_regression_result() {
  _oneof_case_[0] = kRegressionResult;
}
inline void InferenceResult::clear_regression_result() {
  if (has_regression_result()) {
    if (GetArenaNoVirtual() == NULL) {
      delete result_.regression_result_;
    }
    clear_has_result();
  }
}
inline  const ::tensorflow::serving::RegressionResult& InferenceResult::regression_result() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.InferenceResult.regression_result)
  return has_regression_result()
      ? *result_.regression_result_
      : ::tensorflow::serving::RegressionResult::default_instance();
}
inline ::tensorflow::serving::RegressionResult* InferenceResult::mutable_regression_result() {
  if (!has_regression_result()) {
    clear_result();
    set_has_regression_result();
    result_.regression_result_ = 
      ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::RegressionResult >(
      GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.InferenceResult.regression_result)
  return result_.regression_result_;
}
inline ::tensorflow::serving::RegressionResult* InferenceResult::release_regression_result() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.InferenceResult.regression_result)
  if (has_regression_result()) {
    clear_has_result();
    if (GetArenaNoVirtual() != NULL) {
      ::tensorflow::serving::RegressionResult* temp = new ::tensorflow::serving::RegressionResult(*result_.regression_result_);
      result_.regression_result_ = NULL;
      return temp;
    } else {
      ::tensorflow::serving::RegressionResult* temp = result_.regression_result_;
      result_.regression_result_ = NULL;
      return temp;
    }
  } else {
    return NULL;
  }
}
inline void InferenceResult::set_allocated_regression_result(::tensorflow::serving::RegressionResult* regression_result) {
  clear_result();
  if (regression_result) {
    if (GetArenaNoVirtual() != NULL &&
        ::google::protobuf::Arena::GetArena(regression_result) == NULL) {
      GetArenaNoVirtual()->Own(regression_result);
    } else if (GetArenaNoVirtual() !=
               ::google::protobuf::Arena::GetArena(regression_result)) {
      ::tensorflow::serving::RegressionResult* new_regression_result = 
          ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::RegressionResult >(
          GetArenaNoVirtual());
      new_regression_result->CopyFrom(*regression_result);
      regression_result = new_regression_result;
    }
    set_has_regression_result();
    result_.regression_result_ = regression_result;
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.InferenceResult.regression_result)
}
inline  ::tensorflow::serving::RegressionResult* InferenceResult::unsafe_arena_release_regression_result() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.InferenceResult.regression_result)
  if (has_regression_result()) {
    clear_has_result();
    ::tensorflow::serving::RegressionResult* temp = result_.regression_result_;
    result_.regression_result_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline  void InferenceResult::unsafe_arena_set_allocated_regression_result(::tensorflow::serving::RegressionResult* regression_result) {
  clear_result();
  if (regression_result) {
    set_has_regression_result();
    result_.regression_result_ = regression_result;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.InferenceResult.regression_result)
}

inline bool InferenceResult::has_result() const {
  return result_case() != RESULT_NOT_SET;
}
inline void InferenceResult::clear_has_result() {
  _oneof_case_[0] = RESULT_NOT_SET;
}
inline InferenceResult::ResultCase InferenceResult::result_case() const {
  return InferenceResult::ResultCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// MultiInferenceRequest

// repeated .tensorflow.serving.InferenceTask tasks = 1;
inline int MultiInferenceRequest::tasks_size() const {
  return tasks_.size();
}
inline void MultiInferenceRequest::clear_tasks() {
  tasks_.Clear();
}
inline const ::tensorflow::serving::InferenceTask& MultiInferenceRequest::tasks(int index) const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.MultiInferenceRequest.tasks)
  return tasks_.Get(index);
}
inline ::tensorflow::serving::InferenceTask* MultiInferenceRequest::mutable_tasks(int index) {
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.MultiInferenceRequest.tasks)
  return tasks_.Mutable(index);
}
inline ::tensorflow::serving::InferenceTask* MultiInferenceRequest::add_tasks() {
  // @@protoc_insertion_point(field_add:tensorflow.serving.MultiInferenceRequest.tasks)
  return tasks_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::tensorflow::serving::InferenceTask >*
MultiInferenceRequest::mutable_tasks() {
  // @@protoc_insertion_point(field_mutable_list:tensorflow.serving.MultiInferenceRequest.tasks)
  return &tasks_;
}
inline const ::google::protobuf::RepeatedPtrField< ::tensorflow::serving::InferenceTask >&
MultiInferenceRequest::tasks() const {
  // @@protoc_insertion_point(field_list:tensorflow.serving.MultiInferenceRequest.tasks)
  return tasks_;
}

// .tensorflow.serving.Input input = 2;
inline bool MultiInferenceRequest::has_input() const {
  return this != internal_default_instance() && input_ != NULL;
}
inline void MultiInferenceRequest::clear_input() {
  if (GetArenaNoVirtual() == NULL && input_ != NULL) delete input_;
  input_ = NULL;
}
inline const ::tensorflow::serving::Input& MultiInferenceRequest::input() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.MultiInferenceRequest.input)
  return input_ != NULL ? *input_
                         : *::tensorflow::serving::Input::internal_default_instance();
}
inline ::tensorflow::serving::Input* MultiInferenceRequest::mutable_input() {
  
  if (input_ == NULL) {
    _slow_mutable_input();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.MultiInferenceRequest.input)
  return input_;
}
inline ::tensorflow::serving::Input* MultiInferenceRequest::release_input() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.MultiInferenceRequest.input)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_input();
  } else {
    ::tensorflow::serving::Input* temp = input_;
    input_ = NULL;
    return temp;
  }
}
inline  void MultiInferenceRequest::set_allocated_input(::tensorflow::serving::Input* input) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete input_;
  }
  if (input != NULL) {
    _slow_set_allocated_input(message_arena, &input);
  }
  input_ = input;
  if (input) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.MultiInferenceRequest.input)
}

// -------------------------------------------------------------------

// MultiInferenceResponse

// repeated .tensorflow.serving.InferenceResult results = 1;
inline int MultiInferenceResponse::results_size() const {
  return results_.size();
}
inline void MultiInferenceResponse::clear_results() {
  results_.Clear();
}
inline const ::tensorflow::serving::InferenceResult& MultiInferenceResponse::results(int index) const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.MultiInferenceResponse.results)
  return results_.Get(index);
}
inline ::tensorflow::serving::InferenceResult* MultiInferenceResponse::mutable_results(int index) {
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.MultiInferenceResponse.results)
  return results_.Mutable(index);
}
inline ::tensorflow::serving::InferenceResult* MultiInferenceResponse::add_results() {
  // @@protoc_insertion_point(field_add:tensorflow.serving.MultiInferenceResponse.results)
  return results_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::tensorflow::serving::InferenceResult >*
MultiInferenceResponse::mutable_results() {
  // @@protoc_insertion_point(field_mutable_list:tensorflow.serving.MultiInferenceResponse.results)
  return &results_;
}
inline const ::google::protobuf::RepeatedPtrField< ::tensorflow::serving::InferenceResult >&
MultiInferenceResponse::results() const {
  // @@protoc_insertion_point(field_list:tensorflow.serving.MultiInferenceResponse.results)
  return results_;
}

#endif  // !PROTOBUF_INLINE_NOT_IN_HEADERS
// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------


// @@protoc_insertion_point(namespace_scope)


}  // namespace serving
}  // namespace tensorflow

// @@protoc_insertion_point(global_scope)

#endif  // PROTOBUF_tensorflow_5fserving_2fapis_2finference_2eproto__INCLUDED
